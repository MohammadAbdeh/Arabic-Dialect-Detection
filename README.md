# Tweet-level Arabic Dialect Identification

### January 2021

### Abstract

In this work, we consider the task of tweet-level ADI using the recently proposed QADI dataset [1]. We compare the results of training and testing the following standalone models:
- The base ArabicBERT [2], a pretrained BERT-base language model for Arabic.
- The medium ArabicBERT, a pretrained BERT-medium language model for Arabic.
- A feature engineering approach, using word-level and character-level n-grams to train a multi-layer perceptron (MLP).

We also present a novel pipeline that uses these models and applies various ensemble methods. 

The medium ArabicBERT model showed the best results when trained for 8 epochs among the standalone models with an F1-score of **77.03%**. Our Weighted Soft Voting approach has shown the best results with a macro-averaged F1-score of **78.80%** across 18 classes, achieving a new state-of-the-art result in tweet-level multi-class ADI.

<img src="https://github.com/user-attachments/assets/a644aa16-73ba-43e7-883a-6f5b99e99268">

### Related Work

The interest in this task has significantly increased in the last decade. Early works used language modeling approaches, and many shared tasks were organized to tackle ADI. For example:
- The Discriminating between Similar Languages (DSL) shared task at VarDial2016 focused on identifying Arabic dialects in speech transcripts.
- The MADAR shared task included sub-tasks like identifying city-level and country-level dialects from tweets.

BERT-based models, especially AraBERT, have become state-of-the-art for ADI tasks.

###  Experiment Settings

#### Dataset

The QADI dataset consists of 540k training tweets and 3502 test tweets, covering 18 dialects. In this work, we were only
able to retrieve 481,781 training tweet (approximately 90% of the tweets) due
to unavailability of the rest. The following table shows the distribution of the retrieved tweets over the 18 classes.

| Country | Train  | Test |
|---------|--------|------|
| AE      | 26,951 | 192  |
| BH      | 25,910 | 184  |
| DZ      | 16,890 | 170  |
| EG      | 59,977 | 200  |
| IQ      | 16,537 | 178  |
| JO      | 30,208 | 180  |
| KW      | 45,946 | 190  |
| LB      | 32,752 | 194  |
| LY      | 36,706 | 169  |
| MA      | 10,823 | 178  |
| OM      | 21,023 | 169  |
| PL      | 43,223 | 173  |
| QA      | 33,202 | 198  |
| SA      | 28,764 | 199  |
| SD      | 15,693 | 188  |
| SY      | 16,686 | 194  |
| TN      | 10,398 | 154  |
| YE      | 10,092 | 193  |

#### Baseline

The AraBERT model was fine-tuned on the QADI dataset as a baseline.

### Experiments

Reported results of evaluating the models on the QADI test set. The medium ArabicBERT trained for 8 epochs showed the best standalone performance. Ensemble methods, especially weighted soft voting, improved the results further.

| Model                       | Recall | Precision | F1    | Accuracy |
|-----------------------------|--------|-----------|-------|----------|
| AraBERT [4]                 | -      | -         | 60.60 | -        |
| AraBERT                     | 73.38  | 75.20     | 73.30 | 73.27    |
| MLP                         | 51.43  | 55.48     | 49.67 | 51.13    |
| Base ArabicBERT (4 epochs)  | 75.55  | 77.12     | 75.53 | 75.45    |
| Medium ArabicBERT (4 epochs)| 57.19  | 58.13     | 56.04 | 57.13    |
| Medium ArabicBERT (8 epochs)| 77.07  | 78.01     | 77.03 | 76.99    |
| Hard Majority Voting        | 76.91  | 78.39     | 76.85 | 76.81    |
| Unweighted Soft Voting      | 78.24  | 79.92     | 78.20 | 78.17    |
| Weighted Soft Voting        | 78.88  | 80.34     | 78.80 | 78.81    |


### Discussion

The Classification errors can be categorized into the following categories:
-   Classification error generated by the misclassification of geographically close country dialects, such as countries existing in the same region.
-   Classification error generated by the misclassification of dialects across all regions.

<img width = "500" alt="ArabicBERT medium confusion matrix" src="https://github.com/user-attachments/assets/45057e85-6397-4d43-95d6-c519c0741b9d">


<img width="500" alt="Weighted Soft voting confusion matrix" src="https://github.com/user-attachments/assets/b99909d8-74ce-4e83-9d94-e383df6f6545">

Based on inspecting some misclassified tweets, we argue that this mismatch between neighboring countries is caused by the fact that in some cases, a tweet can be classified accurately by more that one dialect. For instance, the tweet “هذا اللي يقولك يموت ولا ينسى” “This is what they say he dies and never forgets” can be classified as any of the Gulf dialects.

By inspecting the misclassified tweets in cases where the misclassified dialect is from another region, we observed that the reason of this mismatch is that the tweets are either too short or include some common phrase that can be used by any Arab . For example: the tweet “ههه الله يهدي البال”, “hhh may god give piece of mind”, includes only a quote used in almost all the Arabic countries regardless of the region.

For a detailed discussion on methods and results, please refer to the [Medium article](https://medium.com/@mohammadabdeh974/tweet-level-arabic-dialect-identification-1f3b2bd6a555).

### References

[1] Younes Samih, Sabit Hassan, Ahmed Abdelali, Hamdy Mubarak, and Kareem Darwish. Arabic Dialect Identification in the Wild. 2020.

[2] Ali Safaya, Moutasem Abdullatif, and Deniz Yuret. KUISAIL at SemEval-2020 Task 12: BERT-CNN for Offensive Speech Identification in Social Media. In Proceedings of the Fourteenth Workshop on Semantic Evaluation, pages 2054–2059, Barcelona (online), December 2020. International Committee for Computational Linguistics.


