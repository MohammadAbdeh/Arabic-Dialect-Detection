{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "JnS4d_tGEW1l",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c251b60b-f7a1-48b0-b108-397e8bbe4a70"
      },
      "source": [
        "!pip install scitime\n",
        "!pip install pyarabic\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!git clone https://github.com/aub-mind/arabert/\n",
        "!cd arabert && git checkout 6a58ca118911ef311cbe8cdcdcc1d0360"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting scitime\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/72/89/70f693c181ce1bc607765dde514202e26bfe165a54fff2c9a2ac85f41814/scitime-0.0.2-py3-none-any.whl (45.9MB)\n",
            "\u001b[K     |████████████████████████████████| 45.9MB 93kB/s \n",
            "\u001b[?25hRequirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.6/dist-packages (from scitime) (0.22.2.post1)\n",
            "Requirement already satisfied: pandas>=0.20.3 in /usr/local/lib/python3.6/dist-packages (from scitime) (1.1.5)\n",
            "Requirement already satisfied: joblib>=0.12.5 in /usr/local/lib/python3.6/dist-packages (from scitime) (1.0.0)\n",
            "Requirement already satisfied: psutil>=5.4.7 in /usr/local/lib/python3.6/dist-packages (from scitime) (5.4.8)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scitime) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.19.1->scitime) (1.4.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->scitime) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.20.3->scitime) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas>=0.20.3->scitime) (1.15.0)\n",
            "Installing collected packages: scitime\n",
            "Successfully installed scitime-0.0.2\n",
            "Collecting pyarabic\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7b/e2/46728ec2f6fe14970de5c782346609f0636262c0941228f363710903aaa1/PyArabic-0.6.10.tar.gz (108kB)\n",
            "\u001b[K     |████████████████████████████████| 112kB 6.3MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyarabic\n",
            "  Building wheel for pyarabic (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyarabic: filename=PyArabic-0.6.10-cp36-none-any.whl size=113324 sha256=192b8c597ba4d8ad97d4b8ba92a51bbc009b3f093b87bc54e0cddef581a5f7df\n",
            "  Stored in directory: /root/.cache/pip/wheels/10/b8/f5/b7c1a50e6efb83544844f165a9b134afe7292585465e29b61d\n",
            "Successfully built pyarabic\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.10\n",
            "Collecting farasapy\n",
            "  Downloading https://files.pythonhosted.org/packages/ba/49/a1cea02059325e99bbd1114704140101893c629e67b15eaacb93711dc91e/farasapy-0.0.11-py3-none-any.whl\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from farasapy) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from farasapy) (4.41.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->farasapy) (1.24.3)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.11\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.6/dist-packages (0.6.10)\n",
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 200, done.\u001b[K\n",
            "remote: Counting objects: 100% (200/200), done.\u001b[K\n",
            "remote: Compressing objects: 100% (147/147), done.\u001b[K\n",
            "remote: Total 414 (delta 94), reused 148 (delta 47), pack-reused 214\u001b[K\n",
            "Receiving objects: 100% (414/414), 3.82 MiB | 16.11 MiB/s, done.\n",
            "Resolving deltas: 100% (217/217), done.\n",
            "Note: checking out '6a58ca118911ef311cbe8cdcdcc1d0360'.\n",
            "\n",
            "You are in 'detached HEAD' state. You can look around, make experimental\n",
            "changes and commit them, and you can discard any commits you make in this\n",
            "state without impacting any branches by performing another checkout.\n",
            "\n",
            "If you want to create a new branch to retain commits you create, you may\n",
            "do so (now or later) by using -b with the checkout command again. Example:\n",
            "\n",
            "  git checkout -b <new-branch-name>\n",
            "\n",
            "HEAD is now at 6a58ca1 added lamb\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "imkC_iTIGLT2",
        "outputId": "5ca68743-a059-4225-9344-3d4a3f39a233"
      },
      "source": [
        "!pip install psutil\n",
        "!pip install joblib"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.8)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (1.0.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9grOigt3k4e",
        "outputId": "437a3d51-86c1-4364-decd-66a052909465"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from scipy.sparse import hstack\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from scitime import Estimator\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.pipeline import FeatureUnion\n",
        "from sklearn.metrics import f1_score, accuracy_score, confusion_matrix, classification_report\n",
        "from farasa.segmenter import FarasaSegmenter\n",
        "from arabert.preprocess_arabert import preprocess\n",
        "\n",
        "\n",
        "farasa_segmenter = FarasaSegmenter(interactive=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 241M/241M [00:22<00:00, 11.5MiB/s]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[2021-01-21 23:35:09,601 - farasapy_logger - WARNING]: Be careful with large lines as they may break on interactive mode. You may switch to Standalone mode for such cases.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GOI0NlC93MsW",
        "outputId": "f52090ae-d5a8-4c13-a11d-5811d355d33b"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "DATA_COLUMN = 'line'\n",
        "LABEL_COLUMN = 'dialect'\n",
        "train_files = [\"/content/drive/MyDrive/trainset/AE_targe_preprocessed.txt\", \"/content/drive/MyDrive/trainset/BH_targe_preprocessed.txt\",\"/content/drive/MyDrive/trainset/DZ_targe_preprocessed.txt\",\n",
        "               \"/content/drive/MyDrive/trainset/EG_targe_preprocessed.txt\", \"/content/drive/MyDrive/trainset/IQ_targe_preprocessed.txt\",\"/content/drive/MyDrive/trainset/JO_targe_preprocessed.txt\",\n",
        "               \"/content/drive/MyDrive/trainset/KW_targe_preprocessed.txt\", \"/content/drive/MyDrive/trainset/LB_targe_preprocessed.txt\",\"/content/drive/MyDrive/trainset/LY_targe_preprocessed.txt\",\n",
        "               \"/content/drive/MyDrive/trainset/QA_targe_preprocessed.txt\", \"/content/drive/MyDrive/trainset/OM_targe_preprocessed.txt\",\"/content/drive/MyDrive/trainset/PL_targe_preprocessed.txt\",\n",
        "               \"/content/drive/MyDrive/trainset/OM_targe_preprocessed.txt\", \"/content/drive/MyDrive/trainset/SA_targe_preprocessed.txt\",\"/content/drive/MyDrive/trainset/SD_targe_preprocessed.txt\",\n",
        "               \"/content/drive/MyDrive/trainset/SY_targe_preprocessed.txt\", \"/content/drive/MyDrive/trainset/TN_targe_preprocessed.txt\",\"/content/drive/MyDrive/trainset/YE_targe_preprocessed.txt\"]\n",
        "\n",
        "country_names = [\"AE\",\"BH\",\"DZ\",\"EG\",\"IQ\",\"JO\",\"KW\",\"LB\",\"LY\",\"MA\",\"OM\",\"PL\",\"QA\",\"SA\",\"SD\",\"SY\",\"TN\",\"YE\"]\n",
        "\n",
        "label_map = {\n",
        "    'AE' : 0,\n",
        "    'BH' : 1,\n",
        "    'DZ' : 2,\n",
        "    'EG' : 3,\n",
        "    'IQ' : 4,\n",
        "    'JO' : 5,\n",
        "    'KW' : 6,\n",
        "    'LB' : 7,\n",
        "    'LY' : 8,\n",
        "    'MA' : 9,\n",
        "    'OM' : 10,\n",
        "    'PL' : 11,\n",
        "    'QA' : 12,\n",
        "    'SA' : 13,\n",
        "    'SD' : 14,\n",
        "    'SY' : 15,\n",
        "    'TN' : 16,\n",
        "    'YE' : 17\n",
        "}\n",
        "\n",
        "print(train_files)\n",
        "train_data_list = []\n",
        "for index in range(len(train_files)):\n",
        "  train_data_list.append(pd.read_csv(train_files[index], lineterminator= '~', header=None, names = ['line'], nrows = 100))\n",
        "  train_data_list[index][LABEL_COLUMN] = [country_names[index] for i in range(len(train_data_list[index].index))]\n",
        "\n",
        "train_data = pd.concat(train_data_list)\n",
        "\n",
        "test_data = pd.read_csv('/content/drive/MyDrive/testset/QADI_test.txt', sep = '\\t', header=None, lineterminator='\\n')\n",
        "test_data.columns = [DATA_COLUMN, LABEL_COLUMN]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "['/content/drive/MyDrive/trainset/AE_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/BH_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/DZ_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/EG_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/IQ_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/JO_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/KW_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/LB_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/LY_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/QA_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/OM_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/PL_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/OM_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/SA_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/SD_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/SY_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/TN_targe_preprocessed.txt', '/content/drive/MyDrive/trainset/YE_targe_preprocessed.txt']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vh5aF9w23rb5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "866e8098-c12a-40d1-ea9c-f2961386cbf8"
      },
      "source": [
        "test_data.drop(test_data.index[test_data[LABEL_COLUMN] == 'MSA'], inplace = True)\n",
        "\n",
        "train_data[DATA_COLUMN] = train_data[DATA_COLUMN].apply(lambda x: preprocess(x, do_farasa_tokenization=True , farasa=farasa_segmenter, use_farasapy = True))\n",
        "\n",
        "train_data[LABEL_COLUMN] = train_data[LABEL_COLUMN].apply(lambda x: label_map[x])\n",
        "\n",
        "test_data[DATA_COLUMN] = test_data[DATA_COLUMN].apply(lambda x: preprocess(x, do_farasa_tokenization=True , farasa=farasa_segmenter, use_farasapy = True))\n",
        "\n",
        "test_data[LABEL_COLUMN] = test_data[LABEL_COLUMN].apply(lambda x: label_map[x])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r100%|██████████| 241M/241M [00:39<00:00, 11.5MiB/s]"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Aq4_8xop3xrR"
      },
      "source": [
        "train_df = pd.DataFrame({\n",
        "    'id':range(len(train_data)),\n",
        "    'label':train_data[LABEL_COLUMN],\n",
        "    'text': train_data[DATA_COLUMN].replace(r'\\n', ' ', regex=True)\n",
        "})\n",
        "\n",
        "dev_df = pd.DataFrame({\n",
        "    'id':range(len(test_data)),\n",
        "    'label':test_data[LABEL_COLUMN],\n",
        "    'text': test_data[DATA_COLUMN].replace(r'\\n', ' ', regex=True)\n",
        "})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MX9yvqgh5_pz",
        "outputId": "3f2d48a3-792e-4b23-dfba-c64abbbda919"
      },
      "source": [
        "train_text = train_df['text']\n",
        "test_text = dev_df['text']\n",
        "all_text = pd.concat([train_text, test_text])\n",
        "train_text"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [مستخدم] ياخي ال+ مدرب أختار +ه و+ ال+ مدرب دخ...\n",
              "1     شو اللي قاعد يجري ف+ ال+ نصر يا أخو +ه . خسر ا...\n",
              "2     اللي يبحث عن مشكل +ة ال+ وصل راح يحصل +ها ف+ ا...\n",
              "3     [مستخدم] أنا مش معترض على تغيير عامر . اللي دخ...\n",
              "4     تراجع مخيف في مستوى ال+ حارس ال+ كبير ماجد ناص...\n",
              "                            ...                        \n",
              "95    شعور يهلك ل+ ما تكتم ال+ وجع بين ك+ و+ بين نفس...\n",
              "96                [مستخدم] و+ نجهل فوق جهل ال+ جاهل +ين\n",
              "97    [مستخدم] ب+ خير و+ لا في +ه شي جت ل+ +ه سكت +ة...\n",
              "98                   إذا طاح ال+ وطن كثر +ت سكاكين +ه !\n",
              "99    [مستخدم] [مستخدم] [مستخدم] [مستخدم] [مستخدم] [...\n",
              "Name: text, Length: 1800, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuNw9q7P4LyA"
      },
      "source": [
        "# train_word_features = word_vectorizer.transform(train_text.values.astype('U'))\n",
        "# test_word_features = word_vectorizer.transform(test_text.values.astype('U'))\n",
        "# train_word_features"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yTM9pg-akIEZ"
      },
      "source": [
        "union = FeatureUnion([(\"word_tfid\", TfidfVectorizer(\n",
        "                                    sublinear_tf=True,\n",
        "                                    strip_accents='unicode',\n",
        "                                    analyzer='word',\n",
        "                                    token_pattern=r'\\w{1,}',\n",
        "                                    ngram_range=(1, 2),\n",
        "                                    max_features=10000)),\n",
        "                      (\"char_tfid\", TfidfVectorizer(\n",
        "                                    sublinear_tf=True,\n",
        "                                    strip_accents='unicode',\n",
        "                                    analyzer='char',\n",
        "                                    ngram_range=(2, 6),\n",
        "                                    max_features=50000)),\n",
        "                      ])\n",
        "\n",
        "union.fit(all_text.values.astype('U'))\n",
        "features = union.transform(train_text.values.astype('U'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yhqccWTvqPI4"
      },
      "source": [
        "# mlp = MLPClassifier(random_state=1, hidden_layer_sizes= (64,), batch_size = 32, max_iter=5, verbose=True, learning_rate_init=0.0001)\n",
        "# mlp.fit(features, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pgopZGSZdSz7",
        "outputId": "2ebbbdb1-3f30-49c0-b4c8-59defd0e953e"
      },
      "source": [
        "# model.save('/content/drive/MyDrive/models/n-grams_5_epochs')\n",
        "# model.evaluate(test_word_features, test_df['label'])\n",
        "# import pickle\n",
        "# pkl_filename = \"/content/drive/MyDrive/nlp_models/n_grams_20_epochs.sav\"\n",
        "# with open(pkl_filename, 'wb') as file:\n",
        "#    pickle.dump(mlp, file)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0     [مستخدم] ياخي ال+ مدرب أختار +ه و+ ال+ مدرب دخ...\n",
              "1     شو اللي قاعد يجري ف+ ال+ نصر يا أخو +ه . خسر ا...\n",
              "2     اللي يبحث عن مشكل +ة ال+ وصل راح يحصل +ها ف+ ا...\n",
              "3     [مستخدم] أنا مش معترض على تغيير عامر . اللي دخ...\n",
              "4     تراجع مخيف في مستوى ال+ حارس ال+ كبير ماجد ناص...\n",
              "                            ...                        \n",
              "95    شعور يهلك ل+ ما تكتم ال+ وجع بين ك+ و+ بين نفس...\n",
              "96                [مستخدم] و+ نجهل فوق جهل ال+ جاهل +ين\n",
              "97    [مستخدم] ب+ خير و+ لا في +ه شي جت ل+ +ه سكت +ة...\n",
              "98                   إذا طاح ال+ وطن كثر +ت سكاكين +ه !\n",
              "99    [مستخدم] [مستخدم] [مستخدم] [مستخدم] [مستخدم] [...\n",
              "Name: text, Length: 1800, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3jkudR2yKSr"
      },
      "source": [
        "test_features = union.transform(test_text.values.astype('U'))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6GK2NsksUOmQ"
      },
      "source": [
        "# predicted = mlp.predict(test_features)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RcPK3bRTVJKd"
      },
      "source": [
        "# print(predicted[10:50])\n",
        "# print(dev_df[\"label\"][10:50])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-yzajYCzVMjF"
      },
      "source": [
        "# loaded_model = pickle.load(open(pkl_filename, 'rb'))\n",
        "# predicted = loaded_model.predict(test_features)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CZx-5WpdWjb8"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xbrtZKhtXqSb"
      },
      "source": [
        "# print(classification_report(dev_df['label'], predicted))\n",
        "# cf_matrix = confusion_matrix(dev_df['label'], predicted)\n",
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "# df_cm = pd.DataFrame(cf_matrix, index = [\"AE\",\"BH\",\"DZ\",\"EG\",\"IQ\",\"JO\",\"KW\",\"LB\",\"LY\",\"MA\",\"OM\",\"PL\",\"QA\",\"SA\",\"SD\",\"SY\",\"TN\",\"YE\"],\n",
        "#                   columns = [\"AE\",\"BH\",\"DZ\",\"EG\",\"IQ\",\"JO\",\"KW\",\"LB\",\"LY\",\"MA\",\"OM\",\"PL\",\"QA\",\"SA\",\"SD\",\"SY\",\"TN\",\"YE\"])\n",
        "# plt.figure(figsize = (10,7))\n",
        "# sns.heatmap(df_cm, annot=True, fmt='g' )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OqUlHINhYILL"
      },
      "source": [
        "# predicted = loaded_model.predict_proba(test_features)\n",
        "# preds_array = np.array(predicted)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j6Ut24d25Gyr"
      },
      "source": [
        "np.savetxt(\"neural_n_grams.csv\", preds_array, delimiter=\",\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_FbgqAqL59FE",
        "outputId": "2b955ef3-0abc-4f94-9e13-61d685ddf08f"
      },
      "source": [
        "# from sklearn.svm import SVC\n",
        "# svm = SVC(kernel = 'linear', gamma='auto', verbose = True, max_iter = 600)\n",
        "# svm.fit(features, train_df['label'])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[LibSVM]"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "SVC(C=1.0, break_ties=False, cache_size=200, class_weight=None, coef0=0.0,\n",
              "    decision_function_shape='ovr', degree=3, gamma='auto', kernel='linear',\n",
              "    max_iter=600, probability=False, random_state=None, shrinking=True,\n",
              "    tol=0.001, verbose=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "G54kBaaU8Kyi",
        "outputId": "40d31c45-8431-4dd7-d1b2-1bb19942f6d8"
      },
      "source": [
        "predicted = svm.predict(test_features)\n",
        "print(classification_report(dev_df['label'], predicted))\n",
        "cf_matrix = confusion_matrix(dev_df['label'], predicted)\n",
        "macro_f1 = f1_score(dev_df['label'], predicted,average='macro')\n",
        "# macro_precision = precision_score(dev_df['label'], predicted,average='macro')\n",
        "# macro_recall = recall_score(dev_df['label'], predicted,average='macro')\n",
        "acc = accuracy_score(dev_df['label'], predicted)\n",
        "print(\"macro f1: \" + str(macro_f1))\n",
        "print(\"macro precision: \" + str(macro_precision))\n",
        "print(\"macro recall: \" +str(macro_recall))\n",
        "print(\"accuracy: \" +str(acc))\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "df_cm = pd.DataFrame(cf_matrix, index = [\"AE\",\"BH\",\"DZ\",\"EG\",\"IQ\",\"JO\",\"KW\",\"LB\",\"LY\",\"MA\",\"OM\",\"PL\",\"QA\",\"SA\",\"SD\",\"SY\",\"TN\",\"YE\"],\n",
        "                  columns = [\"AE\",\"BH\",\"DZ\",\"EG\",\"IQ\",\"JO\",\"KW\",\"LB\",\"LY\",\"MA\",\"OM\",\"PL\",\"QA\",\"SA\",\"SD\",\"SY\",\"TN\",\"YE\"])\n",
        "plt.figure(figsize = (10,7))\n",
        "sns.heatmap(df_cm, annot=True, fmt='g' )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-27-b8eaf343026f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mpredicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msvm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclassification_report\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mcf_matrix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfusion_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmacro_f1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdev_df\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maverage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'macro'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# macro_precision = precision_score(dev_df['label'], predicted,average='macro')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    592\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    593\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 594\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    595\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    596\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 317\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    318\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    319\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_dense_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_sparse_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    360\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnu\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshrinking\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprobability\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_support\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 362\u001b[0;31m             self.probA_, self.probB_)\n\u001b[0m\u001b[1;32m    363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    364\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_compute_kernel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}